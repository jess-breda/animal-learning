{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Binomial & Multinomial Logistic Regression\n",
    "\n",
    "> Written by Jess Breda September 2023 post lab meeting\n",
    "\n",
    "A question that came up (from Carlos) was if the mutli-class model would do better on L/R trials if it was trained on L,R,V. The goal of this notebook is to implement this comparison.\n",
    "\n",
    "**Initial Steps**:\n",
    "\n",
    "[ ] working with simulated data, figure out the dimensions of the multi-class cost\n",
    "\n",
    "* follow up questions here if needed\n",
    "\n",
    "[ ] working with simulated data, create a binomial class for fitting (from prev code)\n",
    "\n",
    "[ ] validate binomial class finds athena/nick like results with base regressors\n",
    "\n",
    "*  start with single animal, then expand\n",
    "\n",
    "[ ] see what prev_violation regressor does for binomial model\n",
    "\n",
    "[ ] figure out how to make train/test split for the models\n",
    "\t\n",
    "* follow up if diff number of training trials might be an issue\n",
    "\n",
    "* probably don't want to make this perfect now, but long term good to think about & have this information easily stored\n",
    "\t\t\n",
    "[ ] determine what the null model comparison would be (if any?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import sys\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "[\n",
    "    sys.path.append(str(folder))\n",
    "    for folder in pathlib.Path(\"../src/\").iterdir()\n",
    "    if folder.is_dir()\n",
    "]\n",
    "from get_rat_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of multi-class cost\n",
    "\n",
    "In the test eval of the cost function, I want to squash out the third dimension so the model performs better. Eg [1/3 1/3 1/3] should become [1/2 1/2] (or [1/2 1/2 0], not sure yet). Need to focus on figuring out the cost code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegressionComp:\n",
    "    def __init__(self, sigma=None, method=\"BFGS\", disp=True):\n",
    "        self.W = None\n",
    "        self.sigma = sigma\n",
    "        self.method = method\n",
    "        self.disp = disp\n",
    "        self.stored_fits = []\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, Y: np.ndarray):\n",
    "        N, D_w_bias = X.shape\n",
    "        _, C = Y.shape\n",
    "        initial_W_flat = np.zeros(D_w_bias * C)\n",
    "\n",
    "        result = minimize(\n",
    "            fun=self.cost,\n",
    "            x0=initial_W_flat,\n",
    "            args=(X.to_numpy(), Y, self.sigma),\n",
    "            method=self.method,\n",
    "            jac=self._gradient,\n",
    "            options={\"disp\": self.disp},\n",
    "        )\n",
    "\n",
    "        self.W = result.x.reshape(D_w_bias, C)\n",
    "        return self.W\n",
    "\n",
    "    def eval(self, X: pd.DataFrame, Y: np.ndarray):\n",
    "        return self.cost(self.W, X.to_numpy(), Y, sigma=None)\n",
    "\n",
    "    def cost(self, W, X, Y, sigma):\n",
    "        \"\"\"\n",
    "        Compute the negative log-likelihood for multi-class\n",
    "        logistic regression with L2 regularization (or MAP).\n",
    "\n",
    "        params\n",
    "        ------\n",
    "        W : np.ndarray, shape (D + 1, C) or flattened (D+1 * C)\n",
    "        weight matrix, will be in flattened form if in use\n",
    "            for minimize() function\n",
    "        X : pd.DataFrame, shape (N, D + 1)\n",
    "            design matrix with bias column\n",
    "        Y : np.ndarray, shape (N, C), where C = 3\n",
    "            one-hot encoded choice labels for each trial as left,\n",
    "            right or violation\n",
    "        sigma : float (default=None)\n",
    "            standard deviation of Gaussian prior, if None no\n",
    "            regularization is applied\n",
    "\n",
    "        returns\n",
    "        -------\n",
    "        - nll : float\n",
    "            negative log-likelihood\n",
    "        \"\"\"\n",
    "        if len(W.shape) == 1:\n",
    "            W = W.reshape(X.shape[1], Y.shape[1])\n",
    "\n",
    "        logits = X @ W\n",
    "        penalty = (\n",
    "            (1 / (2 * (sigma**2))) * np.trace(W[1:, :].T @ W[1:, :]) if sigma else 0\n",
    "        )\n",
    "        nll = (-np.sum(Y * logits) + np.sum(self.log_sum_exp(logits))) + penalty\n",
    "        return nll\n",
    "\n",
    "    def _gradient(self, W, X, Y, sigma):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the negative log-likelihood for\n",
    "        multi-class logistic regression with L2 regularization (or MAP).\n",
    "\n",
    "        params\n",
    "        ------\n",
    "        W : np.ndarray, shape (D + 1, C) or flattened (D+1 * C)\n",
    "        weight matrix, will be in flattened form if in use\n",
    "        for minimize() function\n",
    "        X : pd.DataFrame, shape (N, D + 1)\n",
    "            design matrix with bias column\n",
    "        Y : np.ndarray, shape (N, C), where C = 3\n",
    "            one-hot encoded choice labels for each trial as left,\n",
    "            right or violation\n",
    "        sigma : float (default=None)\n",
    "            standard deviation of Gaussian prior, if None no\n",
    "            regularization is applied\n",
    "\n",
    "        returns\n",
    "        -------\n",
    "        gradient :  np.ndarray, shape (D+1 * C)\n",
    "            gradient of the negative log-likelihood\n",
    "\n",
    "        \"\"\"\n",
    "        if len(W.shape) == 1:\n",
    "            W = W.reshape(X.shape[1], Y.shape[1])\n",
    "\n",
    "        logits = X @ W\n",
    "        P = self._stable_softmax(logits)\n",
    "\n",
    "        if sigma:\n",
    "            penalty_gradient = W / (sigma**2)\n",
    "        else:\n",
    "            penalty_gradient = np.zeros_like(W)\n",
    "\n",
    "        penalty_gradient[0, :] = 0  # No penalty for bias\n",
    "\n",
    "        gradient = X.T @ (P - Y) + penalty_gradient\n",
    "        return gradient.flatten()\n",
    "\n",
    "    @staticmethod\n",
    "    def log_sum_exp(logits):\n",
    "        max_logits = np.max(logits, axis=1, keepdims=True)\n",
    "        return (\n",
    "            np.log(np.sum(np.exp(logits - max_logits), axis=1, keepdims=True))\n",
    "            + max_logits\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def stable_softmax(logits):\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        return exp_logits / sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiClassLogisticRegressionComp(sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 samples with 4 features and 3 classes\n",
      "W is (5, 3) \n",
      "X is (1000, 5) \n",
      "Y is (1000, 3)\n",
      "W has mean -0.046 and std 1.146\n"
     ]
    }
   ],
   "source": [
    "N = 1000  # Number of samples\n",
    "D = 4  # Number of features\n",
    "C = 3  # Number of classes\n",
    "\n",
    "# Generate random feature values\n",
    "X = np.random.normal(size=(N, D))\n",
    "X_with_bias = np.c_[np.ones(N), X]  # bias column\n",
    "\n",
    "# Generate random true weights (including the bias coefficient)\n",
    "true_W = np.random.normal(loc=0, scale=1, size=(D + 1, C))\n",
    "\n",
    "# Generate target labels (on hot encoded) using multinomial logistic function\n",
    "A = X_with_bias @ true_W\n",
    "P = model.stable_softmax(A)\n",
    "Y = np.array([np.random.multinomial(1, n) for n in P])\n",
    "\n",
    "print(f\"Generated {N} samples with {D} features and {C} classes\")\n",
    "print(f\"W is {true_W.shape} \\nX is {X_with_bias.shape} \\nY is {Y.shape}\")\n",
    "print(f\"W has mean {np.mean(true_W):.3f} and std {np.std(true_W):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36529901, 0.04899202, 0.58570898])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "an_lrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
